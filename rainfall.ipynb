{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "# GRID CAN BE prism, goes back to 1981\n",
    "# if want to go further back, go back to 1950 with nrcc-nn\n",
    "#change nrcc-model to prism or nrcc-nn\n",
    "# if i remove county mean it just gives values and not counties\n",
    "def get_rainfall_data(n,s,e,w,start_date, end_date):\n",
    "    input_dict = {\"bbox\":f\"{w},{s},{e},{n}\",\"sdate\":f\"{start_date}\",\"edate\":f\"{end_date}\",\"grid\":\"nrcc-model\",\"elems\":[{\"name\":\"pcpn\",\"interval\":[0,0,1]}]}\n",
    "    # gets previous 30 days including current day and sums it = {\"bbox\":f\"{w},{s},{e},{n}\",\"date\":\"2008-07-30\",\"grid\":\"nrcc-model\",\"elems\":[{\"name\":\"pcpn\",\"interval\":[1,0,0],\"duration\":30,\"reduce\":\"sum\"}]}\n",
    "    #input_dict = {\"bbox\":\"-75,40,-74,41\",\"sdate\":\"2023-07-01\",\"edate\":\"2023-07-07\",\"grid\":\"nrcc-model\",\"elems\":[{\"name\":\"pcpn\",\"interval\":[0,0,1],\"area_reduce\":\"county_mean\"}]}\n",
    "    req = requests.post(\"http://grid2.rcc-acis.org/GridData\", json = input_dict)\n",
    "    data = req.json()\n",
    "    return data\n",
    "\n",
    "def get_rainfall_data_avg(n,s,e,w, end_date, window):\n",
    "    input_dict = {\"bbox\":f\"{w},{s},{e},{n}\",f\"{end_date}\":\"2008-07-30\",\"grid\":\"nrcc-model\",\"elems\":[{\"name\":\"pcpn\",\"interval\":[1,0,0],\"duration\":window,\"reduce\":\"sum\"}]}\n",
    "    req = requests.post(\"http://grid2.rcc-acis.org/GridData\", json = input_dict)\n",
    "    data = req.json()\n",
    "    return data\n",
    "\n",
    "def avg_rainfall_data(data):\n",
    "    info = {}\n",
    "    for dates in data[\"data\"]:\n",
    "        accum = 0\n",
    "        total = 0\n",
    "        for section in dates[1]:\n",
    "            for item in section:\n",
    "                if item != -999:\n",
    "                    total += item\n",
    "                    accum += 1\n",
    "        if accum != 0:\n",
    "            total /= accum\n",
    "        info[dates[0]] = total\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_comparison(data[], \"date\", \"start_date\", \"end_date\", index_of_site, window_in_wks, if_plotting, percentile) & daily_analysis()\n",
    "# daily_analysis(graph[], start(dt), end(dt), \"date\", \"end_date\", start2(dt))\n",
    "# change in gw levels for one station, sorted from decrease to increase\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "MONTH_DATA = [31,28,31,30,31,30,31,31,30,31,30,31,31]\n",
    "# Avg every day\n",
    "import datetime as dt\n",
    "MON_ACCUM = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]\n",
    "\n",
    "def daily_analysis(graph, start, end, date, end_date, start2):\n",
    "    # returns list of that week average over all years\n",
    "    date_data = []\n",
    "    for date in list(graph.keys()):\n",
    "        #print(str(graph(date)))\n",
    "        date_data.append([dt.datetime(int(date[0:4]),int(date[5:7]),int(date[8:10])), graph[date]])\n",
    "    total_data = []\n",
    "    while start.year < int(end_date):\n",
    "        lst = []\n",
    "        accum = 0\n",
    "        for i in date_data:\n",
    "            if i[0] >= start and i[0] <= end:\n",
    "                lst.append(i)\n",
    "                accum += i[1]\n",
    "        if len(lst):\n",
    "            total_data.append([start2.year, accum / len(lst)])\n",
    "        #if start.year % 4 == 0 and start.month == 2:\n",
    "            # LEAP YEAR\n",
    "        start = dt.datetime(start.year + 1, start.month, start.day)\n",
    "        start2 = dt.datetime(start2.year + 1, start2.month, start2.day)\n",
    "        end = dt.datetime(end.year + 1, end.month, end.day)\n",
    "    return total_data\n",
    "\n",
    "def daily_no_avg(graph, start, date, end_date, start2, scope, typ):\n",
    "    # returns list of that week average over all years\n",
    "    date_data = []\n",
    "    for date in list(graph.keys()):\n",
    "        #print(str(graph(date)))\n",
    "        date_data.append([dt.datetime(int(date[0:4]),int(date[5:7]),int(date[8:10])), graph[date]])\n",
    "    total_data = []\n",
    "    # just take first day and next first day\n",
    "    while start.year < int(end_date):\n",
    "        #print(f'start is {start}')\n",
    "        lst2 = []\n",
    "        accum = 0\n",
    "        min = 1000000\n",
    "        for i in range(len(date_data)):\n",
    "            #print(date_data[i])\n",
    "            #print(start)\n",
    "            #print(date_data[i][0])\n",
    "            #print(start)\n",
    "            if date_data[i][0] >= start - dt.timedelta(days=scope) and date_data[i][0] <= start + dt.timedelta(days=scope):\n",
    "                #print(date_data[i][0])\n",
    "                lst2.append(date_data[i])\n",
    "                if typ == 'avg':\n",
    "                    #print(date_data[i][1])\n",
    "                    accum += date_data[i][1]\n",
    "                elif typ == \"min\" and date_data[i][1] < min:\n",
    "                    min = date_data[i][1]\n",
    "        if len(lst2) >= 2:\n",
    "            if typ == \"avg\":\n",
    "                total_data.append([start2.year, accum/len(lst2)])\n",
    "            elif typ == \"min\":\n",
    "                total_data.append([start2.year, min])\n",
    "        #if start.year % 4 == 0 and start.month == 2:\n",
    "            # LEAP YEAR\n",
    "        start = dt.datetime(start.year + 1, start.month, start.day)\n",
    "        start2 = dt.datetime(start2.year + 1, start2.month, start2.day)\n",
    "    return total_data\n",
    "\n",
    "def sortkey2(e):\n",
    "    return e[1]\n",
    "\n",
    "def graph_comparison(graph, date, start_date, end_date, index_of_site, window, plotting, percentile, scope, typ):\n",
    "    if len(date) == 5:\n",
    "        start = dt.datetime(int(start_date), int(date[0:2]), int(date[3:5]))\n",
    "        start2 = start + dt.timedelta(weeks=window * 2)\n",
    "        end2 = start + dt.timedelta(weeks=3 * window,days=-1)\n",
    "    #print(graph[0])\n",
    "    #print(graph[1])\n",
    "    first_set = daily_no_avg(graph[0], start, date, end_date, start, scope, typ)\n",
    "    second_set = daily_no_avg(graph[1], start2, date, end_date, start, scope, typ)\n",
    "    #print(first_set)\n",
    "    #print(second_set)\n",
    "    plottable = []\n",
    "    for i in first_set:\n",
    "        for j in second_set:\n",
    "             if j[0] == i[0]:\n",
    "                  plottable.append([i[0], j[1] - i[1]])\n",
    "    dct = {}\n",
    "    plottable.sort(key=sortkey2)\n",
    "    for i in plottable:\n",
    "        if plotting:\n",
    "            dct[(str(i[0]))[2:4]] = i[1]\n",
    "        else:\n",
    "            dct[(str(i[0]))] = i[1]\n",
    "    if plotting:\n",
    "        plt.bar(dct.keys(), dct.values(), color = \"green\", width = 0.8)\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(\"Rainfall (%s) versus period after\" % (date))\n",
    "        plt.show()\n",
    "        #for i in dct.keys():\n",
    "            #print(f'{i}: {dct[i]:.2f}')\n",
    "    else:\n",
    "        #print(f'dct is {dct}')\n",
    "        if len(dct) > 0:\n",
    "            if percentile != 0:\n",
    "                a = list(dct.values())\n",
    "                a = np.array(a)\n",
    "                p = np.percentile(a, percentile)  # return 50th percentile, i.e. median.\n",
    "                return p\n",
    "            else:\n",
    "                # [mean, median, interquartile range,standard deviation]\n",
    "                b = list(dct.values())\n",
    "                b = np.array(b)\n",
    "                return [list(dct.values()), np.mean(b), np.median(b), np.percentile(b, 75) - np.percentile(b, 25), np.std(b),start.strftime(\"%m/%d\"), end2.strftime(\"%m/%d\"), list(dct.keys())]\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using_all_weeks(data[], \"date\", \"start_date\", \"end_date\", index_of_site, percent_of_total_outputted)\n",
    "def sortkey(e):\n",
    "    return e[0]\n",
    "\n",
    "def using_all_weeks(data, date, start_date, end_date, index_of_site, percent, scope, typ):\n",
    "    graph = data\n",
    "    date = dt.datetime(int(start_date), int(date[0:2]), int(date[3:5]))\n",
    "    window = 0.5\n",
    "    total_data = []\n",
    "    date_data = []\n",
    "    for week in range(int(26 / window) + 1):\n",
    "        date_data.append({})\n",
    "    for stat in graph:\n",
    "        new_date = dt.datetime(int(stat[0:4]),int(stat[5:7]),int(stat[8:10]))\n",
    "        date_data[((new_date.timetuple().tm_yday - 1) // 7)][stat] = graph[stat]\n",
    "    for week in range(int(26 / window)):\n",
    "        date2 = date + dt.timedelta(weeks= week * 2 * window)\n",
    "        # investigate looking across year\n",
    "        #Convert to string\n",
    "        mo = str(date2.month)\n",
    "        da = str(date2.day)\n",
    "        if len(mo) == 1:\n",
    "            mo = \"0\" + mo\n",
    "        if len(da) == 1:\n",
    "            da = '0' + da\n",
    "        #print(graph_comparison(data, f\"{mo}-{da}\", start_date, end_date, index_of_site, window, True, 0))\n",
    "        #graph_comparison(graph, f\"{mo}-{da}\", start_date, end_date, index_of_site, window, True, 0)\n",
    "        info = (graph_comparison([{**date_data[week - 1], **date_data[week], **date_data[week + 1]}, {**date_data[week], **date_data[week + 1]}], f\"{mo}-{da}\", start_date, end_date, index_of_site, window, False, 0, scope, typ))\n",
    "        #print(info)\n",
    "        # Need to gather full date of entry\n",
    "        #ERROR\n",
    "        if info:\n",
    "            for i in range(len(info[0])):\n",
    "                total_data.append([info[0][i],f\"{info[7][i]}/{(info[5])} - {info[7][i]}/{info[6]}\"])\n",
    "    total_data.sort(key=sortkey)\n",
    "    print(str(percent) + \"%\")\n",
    "    for i in range(int(len(total_data) / (100 / percent))):\n",
    "        print(f'{total_data[i][0]:.2f}: {total_data[i][1]}')\n",
    "\n",
    "    print(str(100 - percent) + \"%\")\n",
    "    for i in total_data[-1 * int(len(total_data) / (100 / percent)): -1]:\n",
    "        print(f'{i[0]:.2f}: {i[1]}')\n",
    "    return total_data[0:(int(int(len(total_data) / (100 / percent))))]\n",
    "    #print(\"This data may be skewed because when only looking at one day compared to the next week day, if either day is missing, the\\ndata gets tossed out. Therefore, plenty of years where there are great changes may be left out.\\nThis can be solved by looking at each day of the week, starting at 01-01 to 01-06.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 weeks apart\n",
      "-1.09, 2007/04/29 - 2007/05/13 matches -32000.00, 2007/04/02 - 2007/04/16\n",
      " 2 weeks apart\n",
      "-1.75, 2005/04/01 - 2005/04/22 matches -1733.00, 2005/04/02 - 2005/04/23\n",
      " 3 weeks apart\n",
      " 4 weeks apart\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nill have 3 lists of dates, how do i sort?\\nsort but still keep separate?\\n\\niterate through one list type:\\n    if month and year match, print all of those saying streamflow or evap or rainfall\\n    \\ngo to end of one list\\n\\n\\nfrom rainfall:\\nfor first time period, want 30 day that ends on the day the first chunk ends. Not precip for 30 days, but departure from normal (will get a call for that)\\n30 day period is bucket, bucket is normally that full, but this year how much from normal fullness\\n\\nsummation of 30 days in year i - normal\\n\\n30 days 60 days or 90 days\\nuse same bounding boxes as clusters for stream flow analysis\\n\\ntop 5 big dates\\nother period, summation of 30 days (gap can be 1 week or 2 weeks or 3 weeks apart) (2 or 3 weeks mostly)\\njan30-feb30-mar30-ect for all time\\nstream flow avg over 7 days\\n\\n2%\\n7 day avg change with next 7 days: sum of all rainfall of last 30 days ending on end of 7 day avg\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver\n",
    "#avg_rainfall_data(get_rainfall_data(41,40,-73,-74,\"2019-12-15\",\"2020-1-23\"))\n",
    "#data = get_rainfall_data(41,40,-73,-74,\"1950-01-01\",\"2022-12-31\")\n",
    "#data = avg_rainfall_data(data)\n",
    "#using_all_weeks(data, \"01-01\", \"1950\",\"2022\", 0, 2, 2, \"avg\")\n",
    "\n",
    "'''\n",
    "\n",
    "ill have 3 lists of dates, how do i sort?\n",
    "sort but still keep separate?\n",
    "\n",
    "iterate through one list type:\n",
    "    if month and year match, print all of those saying streamflow or evap or rainfall\n",
    "    \n",
    "go to end of one list\n",
    "\n",
    "\n",
    "from rainfall:\n",
    "for first time period, want 30 day that ends on the day the first chunk ends. Not precip for 30 days, but departure from normal (will get a call for that)\n",
    "30 day period is bucket, bucket is normally that full, but this year how much from normal fullness\n",
    "\n",
    "summation of 30 days in year i - normal\n",
    "\n",
    "30 days 60 days or 90 days\n",
    "use same bounding boxes as clusters for stream flow analysis\n",
    "\n",
    "top 5 big dates\n",
    "other period, summation of 30 days (gap can be 1 week or 2 weeks or 3 weeks apart) (2 or 3 weeks mostly)\n",
    "jan30-feb30-mar30-ect for all time\n",
    "stream flow avg over 7 days\n",
    "\n",
    "2%\n",
    "7 day avg change with next 7 days: sum of all rainfall of last 30 days ending on end of 7 day avg\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
