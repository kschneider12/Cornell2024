{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDDI processing code - monthly_eddi_pctl.py  \n",
    "## Used by monthly_eddi_pctl.py and offmonthly_eddi_pctl.py\n",
    "## Last updated: 2024-08-12\n",
    "## Keith Eggleston and Kent Schneider\n",
    "\n",
    "import sys, requests, calendar, json\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "#from local_defs import directories\n",
    "\n",
    "# Directory for input files\n",
    "indir = \"indir/\"\n",
    "#indir = directories[\"indir\"]\n",
    "\n",
    "area_names = {\n",
    "\t'Gridpoints': {'code': 1}\n",
    "}\n",
    "\n",
    "\n",
    "def process_eddi3 (ryear, rmonth, rday):\n",
    "\t# check if a file exists for requested day (last day of month), otherwise last day available (within 10 days)\n",
    "\tcheck_date = datetime(ryear, rmonth, rday)\n",
    "\tlast_to_check = check_date - timedelta(days=10)\n",
    "\tuse_date = None\n",
    "\twhile check_date >= last_to_check:\n",
    "\t\tyyyymmdd = check_date.strftime(\"%Y%m%d\")\n",
    "\t\tfilename = \"EDDI_ETrs_02mn_{}.asc\".format(yyyymmdd)\n",
    "\t\tprint ('Checking for',filename)\n",
    "\t\turl = \"https://downloads.psl.noaa.gov/Projects/EDDI/CONUS_archive/data/{0}/{1}\".format(check_date.year, filename)\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tif response.ok:\n",
    "\t\t\tuse_date = check_date\n",
    "\t\t\tbreak\n",
    "\t\tcheck_date = check_date - timedelta(days=1)\n",
    "\n",
    "\tif not use_date:\n",
    "\t\tprint ('No data within 10 days of requested date; exiting...')\n",
    "\t\tsys.exit()\n",
    "\telse:\n",
    "\t\tprint (\"Current year data file is \", use_date.strftime(\"%Y-%m-%d\"))\n",
    "\t\tryear = use_date.year\n",
    "\t\trmonth = use_date.month\n",
    "\t\trday = use_date.day\n",
    "\n",
    "\t#####################################\n",
    "\t### load EDDI data from files\n",
    "\t######################################\n",
    "\n",
    "\ttoday = datetime.today()\n",
    "\tgridpt_eddi = {\n",
    "\t\t\"data\": {},\n",
    "\t\t\"date\": str(today),\n",
    "\t\t\"data_thru\": \"{0}\".format(use_date.strftime(\"%Y-%m-%d\"))\n",
    "\t}\n",
    "\tfor area in area_names.keys():\n",
    "\t\tgridpt_eddi[\"data\"][area] = {}\n",
    "\n",
    "\t#basin_file = open('{}ma_basin_grids.json'.format(indir), 'r')\n",
    "\t#ma_basin_grids = json.load(basin_file)\n",
    "\t#region_file = open('{}ma_region_grids.json'.format(indir), 'r')\n",
    "\t#ma_region_grids = json.load(region_file)\n",
    "\tny_file = open('{}ny_grids.json'.format(indir), 'r')\n",
    "\tny_grids = json.load(ny_file)\n",
    "\n",
    "\n",
    "\t#for timescale in ['02mn', '01mn']:\t#order is important for populating dictionaries below\n",
    "\tfor timescale in ['01mn']:\n",
    "\t\tprint (timescale,'- Loading data from files')\n",
    "\t\tarea_vals = {}\n",
    "\t\tfor area in area_names.keys():\n",
    "\t\t\tarea_vals[area] = {}\n",
    "\n",
    "\t\tfor y in range(1980, ryear+1):\n",
    "\t\t\t# period with bad forcing re: Mike Hobbins \n",
    "\t\t\tif (y == 2022 and rmonth >= 8) or (y == 2023 and rmonth <= 7):\n",
    "\t\t\t\tprint ('Suspect data; skipping {0}-{1}'.format(y,rmonth))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif rmonth == 2 and rday == 29 and not calendar.isleap(y):\n",
    "\t\t\t\ttday = 28\n",
    "\t\t\telse:\n",
    "\t\t\t\ttday = rday\n",
    "\t\t\tyyyymmdd = \"%s%02d%02d\" % (y,rmonth,tday)\n",
    "\n",
    "\t\t\t#print (yyyymmdd)\n",
    "\t\t\tfilename = \"EDDI_ETrs_{0}_{1}.asc\".format(timescale,yyyymmdd)\n",
    "\t\t\turl = \"https://downloads.psl.noaa.gov/Projects/EDDI/CONUS_archive/data/{0}/{1}\".format(y,filename)\n",
    "\t\t\tresponse = requests.get(url)\n",
    "\t\t\tif response.ok:\n",
    "\t\t\t\tifile = response.text\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint (filename,\"; bad status code:\",response.status_code,'; skipping ...')\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlinesOfFile = ifile.split('\\n')\n",
    "\n",
    "\t\t\t#information lines\n",
    "\t\t\tnlon = int(linesOfFile[0].split()[-1])\n",
    "\t\t\tnlat = int(linesOfFile[1].split()[-1])\n",
    "\t\t\tll_lon = float(linesOfFile[2].split()[-1])\n",
    "\t\t\tll_lat = float(linesOfFile[3].split()[-1])\n",
    "\t\t\tres = float(linesOfFile[4].split()[-1])\n",
    "\t\t\tmiss = float(linesOfFile[5].split()[-1])\n",
    "\n",
    "\t\t\t#check for errors or changes to file header info\n",
    "\t\t\tif nlon != 464:\n",
    "\t\t\t\tprint ('Wrong nlon', nlon)\n",
    "\t\t\tif nlat != 224:\n",
    "\t\t\t\tprint ('Wrong nlat', nlat)\n",
    "\t\t\tif ll_lon != -125.0:\n",
    "\t\t\t\tprint ('Wrong ll_lon', ll_lon)\n",
    "\t\t\tif ll_lat != 25.0:\n",
    "\t\t\t\tprint ('Wrong ll_lat', ll_lat)\n",
    "\t\t\tif res != 0.125:\n",
    "\t\t\t\tprint ('Wrong res', res)\n",
    "\n",
    "\t\t\t#lines of data\n",
    "\t\t\tdata_list = []\n",
    "\t\t\tfor line in linesOfFile[6:]:\n",
    "\t\t\t\tif len(line) > 0:\n",
    "\t\t\t\t\tdata_list.append(line.split())\n",
    "\n",
    "\t\t\t#check for correct shape\n",
    "\t\t\tif len(data_list) != 224:\n",
    "\t\t\t\tprint ('Wrong number of lines od data', len(data_list))\n",
    "\t\t\tif len(data_list[0]) != 464:\n",
    "\t\t\t\tprint ('Wrong number of columns of data', len(data_list[0]))\n",
    "\n",
    "\t\t\t#extract data for each grid points in specified REGION\n",
    "\t\t\tfor region in ny_grids.keys():\n",
    "\t\t\t\tpt = 0\n",
    "\t\t\t\tfor coor in ny_grids[region]:\n",
    "\t\t\t\t\tif not pt in area_vals[region].keys():\n",
    "\t\t\t\t\t\tarea_vals[region][pt] = []\n",
    "\t\t\t\t\tval = float(data_list[coor[3]][coor[2]])\t#x,y coordinates in grid\n",
    "\t\t\t\t\tarea_vals[region][pt].append([yyyymmdd,val * -1])\t#reverse sign so negative values are dry, instead of wet\n",
    "\t\t\t\t\tpt += 1\n",
    "\n",
    "\t\t\t#extract data for each grid points in specified BASIN\n",
    "\n",
    "\t\t# perform calculations using retrieved data\n",
    "\t\tfor region in ny_grids.keys():\n",
    "\t\t\tpt = 0\n",
    "\t\t\tdct = {}\n",
    "\t\t\tfor coor in range(len(area_vals[region])):\n",
    "\t\t\t\thist = area_vals[region][coor][:-1]\n",
    "\t\t\t\tfor dataset in hist:\n",
    "\t\t\t\t\t#print(dataset)\n",
    "\t\t\t\t\tif f'{dataset[0][:4]}-{dataset[0][4:6]}-{dataset[0][6:]}' in dct.keys():\n",
    "\t\t\t\t\t\tdct[f'{dataset[0][:4]}-{dataset[0][4:6]}-{dataset[0][6:]}'] += dataset[1]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t#print(f'{dataset[0][:4]}-{dataset[0][4:6]}-{dataset[0][6:]} not in dct')\n",
    "\t\t\t\t\t\tdct[f'{dataset[0][:4]}-{dataset[0][4:6]}-{dataset[0][6:]}'] = dataset[1]\n",
    "\t\t\t\t\t#print(dct.keys())\n",
    "\t\t\t\t\t#print(dct)\n",
    "\t\t\tfor item in dct:\n",
    "\t\t\t\t#print(item)\n",
    "\t\t\t\t#print(len(area_vals[region]))\n",
    "\t\t\t\tdct[item] = dct[item] / len((area_vals[region]))\n",
    "\t\t\t\t#print(dct)\n",
    "\t\t\treturn(dct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_all_data(end_date)\n",
    "\n",
    "import datetime as dt\n",
    "def get_all_data(end_date):\n",
    "    data = {}\n",
    "    year = 1980\n",
    "    start_date = dt.datetime(year, 1, 1)\n",
    "    # Iterating through each day of the year\n",
    "    #91 IS APRIL 1\n",
    "    for i in range(91,274):\n",
    "        if i == 59:\n",
    "            continue\n",
    "        current_date = start_date + dt.timedelta(days=i)\n",
    "        data.update(process_eddi3(end_date, current_date.month, current_date.day))\n",
    "        print(f\"{current_date.strftime('%m/%d')} completed\")\n",
    "        outfilename = '%sall_data_saved.json'%(outdir)\n",
    "        outfile = open(outfilename, \"w\")\n",
    "        json.dump(data, outfile)\n",
    "        outfile.close()\n",
    "    ## Write regional results to JSON file - 'regional_eddi_[year]-[month].json'\n",
    "    outfilename = '%sall_data_saved.json'%(outdir)\n",
    "    outfile = open(outfilename, \"w\")\n",
    "    json.dump(data, outfile)\n",
    "    outfile.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_comparison(graph, date, start_date, end_date, window, plotting, percentile, scope)\n",
    "import matplotlib as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "def sortkey2(e):\n",
    "    return e[1]\n",
    "\n",
    "def daily_no_avg(graph, start, date, end_date, start2, scope):\n",
    "    # returns list of that week average over all years\n",
    "    date_data = []\n",
    "    for date in list(graph.keys()):\n",
    "        date_data.append([dt.datetime(int(date[0:4]),int(date[5:7]),int(date[8:10])), graph[date]])\n",
    "    total_data = []\n",
    "    # just take first day and next first day\n",
    "    while start.year < int(end_date):\n",
    "        lst2 = []\n",
    "        accum = 0\n",
    "        min = 1000000\n",
    "        for i in range(len(date_data)):\n",
    "            if date_data[i][0] >= start and date_data[i][0] <= start + dt.timedelta(days=scope):\n",
    "                lst2.append(date_data[i])\n",
    "                accum += date_data[i][1]\n",
    "        if len(lst2) >= 2:\n",
    "            total_data.append([start2.year, accum/len(lst2)])\n",
    "        start = dt.datetime(start.year + 1, start.month, start.day)\n",
    "        start2 = dt.datetime(start2.year + 1, start2.month, start2.day)\n",
    "    return total_data\n",
    "\n",
    "\n",
    "def graph_comparison(graph, date, start_date, end_date, window, plotting, percentile, scope):\n",
    "    if len(date) == 5:\n",
    "        start = dt.datetime(int(start_date), int(date[0:2]), int(date[3:5]))\n",
    "        start2 = start + dt.timedelta(weeks=window)\n",
    "        end2 = start2 + dt.timedelta(days=scope)\n",
    "    first_set = daily_no_avg(graph[0], start, date, end_date, start, scope)\n",
    "    second_set = daily_no_avg(graph[1], start2, date, end_date, start, scope)\n",
    "    plottable = []\n",
    "    for i in first_set:\n",
    "        for j in second_set:\n",
    "             if j[0] == i[0]:\n",
    "                  if i[1] != 0:\n",
    "                    plottable.append([i[0], j[1] - i[1]])\n",
    "    dct = {}\n",
    "    plottable.sort(key=sortkey2)\n",
    "    for i in range(len(plottable)):\n",
    "        if plotting:\n",
    "            dct[(str(plottable[i][0]))[2:4]] = plottable[i][1]\n",
    "        else:\n",
    "            dct[(str(plottable[i][0]))] = plottable[i][1]\n",
    "    if plotting:\n",
    "        plt.bar(dct.keys(), dct.values(), color = \"green\", width = 0.8)\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(\"Streamflow (%s) versus period after\" % (date))\n",
    "        plt.show()\n",
    "    else:\n",
    "        if len(dct) > 0:\n",
    "            if percentile != 0:\n",
    "                a = list(dct.values())\n",
    "                a = np.array(a)\n",
    "                p = np.percentile(a, percentile)  # return 50th percentile, i.e. median.\n",
    "                return p\n",
    "            else:\n",
    "                # [mean, median, interquartile range,standard deviation]\n",
    "                b = list(dct.values())\n",
    "                b = np.array(b)\n",
    "\n",
    "                # Don't need mean, median etc, but is possible\n",
    "                return [list(dct.values()), 0, 0, 0, 0,start.strftime(\"%m/%d\"), end2.strftime(\"%m/%d\"), list(dct.keys())]\n",
    "                #return [list(dct.values()), np.mean(b), np.median(b), np.percentile(b, 75) - np.percentile(b, 25), np.std(b),start.strftime(\"%m/%d\"), end2.strftime(\"%m/%d\"), list(dct.keys())]\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using_all_weeks(date, end_date, percent, scope, window)\n",
    "\n",
    "def sortkey(e):\n",
    "    return e[0]\n",
    "\n",
    "\n",
    "def using_all_weeks(date, end_date, percent, scope, window):\n",
    "    start_date = 1980\n",
    "\n",
    "\n",
    "    # IF YOU ARE GATHERING RAW DATA AND NOT USING FROM A SOURCE, RUN get_all_data(end_date) AS LISTED BELOW\n",
    "    #graph = get_all_data(end_date)\n",
    "\n",
    "    # otherwise, file can be loaded here, change Long Island2.json to name of file\n",
    "    graph = json.load(open('{}Long Island2.json'.format(outdir), 'r'))\n",
    "    date = dt.datetime(start_date, int(date[0:2]), int(date[3:5]))\n",
    "    total_data = []\n",
    "    date_data = []\n",
    "    for week in range(53):\n",
    "        date_data.append({})\n",
    "    for stat in graph:\n",
    "        new_date = dt.datetime(int(stat[0:4]),int(stat[5:7]),int(stat[8:10]))\n",
    "        date_data[((new_date.timetuple().tm_yday - 1) // 7)][stat] = graph[stat]\n",
    "\n",
    "    for week in range(52):\n",
    "        date2 = date + dt.timedelta(weeks=week)\n",
    "        # investigate looking across year\n",
    "        #Convert to string\n",
    "        mo = str(date2.month)\n",
    "        da = str(date2.day)\n",
    "        if len(mo) == 1:\n",
    "            mo = \"0\" + mo\n",
    "        if len(da) == 1:\n",
    "            da = '0' + da\n",
    "        temp1 = {}\n",
    "        temp2 = {}\n",
    "        temp1 = {**temp1,**date_data[week]}\n",
    "        if week + window < 52:\n",
    "            temp2 = {**temp2,**date_data[week + window]}\n",
    "        else:\n",
    "            temp2 = {**temp2,**date_data[(52 - week + window)]}\n",
    "        if scope // 7 > 1:\n",
    "            for additional_week in range((scope // 7) + 1):\n",
    "                if additional_week + week < 53:\n",
    "                    temp1 = {**temp1, **date_data[week + additional_week]}\n",
    "                else:\n",
    "                    temp1 = {**temp1, **date_data[week + additional_week - 52]}\n",
    "                if additional_week + week + (scope // 7) < 52:\n",
    "                    temp2 = {**temp2, **date_data[week + additional_week + (scope // 7)]}\n",
    "                else:\n",
    "                    temp2 = {**temp2, **date_data[week + additional_week - 52 + (scope // 7)]}\n",
    "        # COMBINE NECESSARY WEEKS FOR ANALYSIS\n",
    "        info = (graph_comparison([temp1, temp2], f\"{mo}-{da}\", start_date, end_date, window, False, 0, scope))\n",
    "        if info:\n",
    "            for i in range(len(info[0])):\n",
    "                total_data.append([info[0][i],f\"{info[7][i]}/{(info[5])} - {info[7][i]}/{info[6]}\"])\n",
    "    total_data.sort(key=sortkey)\n",
    "    '''\n",
    "    print(str(percent) + \"%\")\n",
    "    for i in range(int(len(total_data) / (100 / percent))):\n",
    "        print(f'{total_data[i][0][0]:.2f}: ({total_data[i][0][1]:.2f}), {total_data[i][1]}')\n",
    "    '''\n",
    "    templst = []\n",
    "    for i in (total_data[0:10]):\n",
    "        templst.append(f'{i[0]:.2f}, {i[1]}')\n",
    "    print(templst)\n",
    "    return total_data[0:(int(int(len(total_data) / (100 / percent))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing(data, end_date, percentile)\n",
    "\n",
    "MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', \"Dec\"]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graphing(data, end_date, percentile):\n",
    "    year_accum = []\n",
    "    year_accum2 = []\n",
    "    for i in range(int(end_date)-1980):\n",
    "        year_accum.append(0)\n",
    "        year_accum2.append(0)\n",
    "    month_accum = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for lst in data:\n",
    "            month_accum[int(lst[1][5:7]) - 1] += 1\n",
    "\n",
    "    plt.bar(MONTHS, month_accum, color =\"blue\")\n",
    "    plt.title(f\"Evaporation: {percentile} percent between {1980}-{end_date}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Number of Occurances\")\n",
    "    plt.show()\n",
    "    if percentile == 10:\n",
    "        print(f\"1%: {data[int(len(data[0]) / 100)][0]:.2f}\")\n",
    "        print(f\"2%: {data[int(len(data[0]) / 50)][0]:.2f}\")\n",
    "        print(f\"5%: {data[int(len(data) / 20)][0]:.2f}\")\n",
    "        print(f\"10%: {data[int(len(data) / 10)][0]:.2f}\")\n",
    "\n",
    "        for lst in data[0:int(len(data) / (10))]:\n",
    "            year_accum[int(lst[1][0:4]) - int(1980)] += 1\n",
    "\n",
    "        plt.bar((range(int(1980), int(end_date))), year_accum, color =\"green\")\n",
    "        plt.title(f\"Occurances across years: 1 percent between {1980}-{end_date}\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Number of Occurances\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 weeks\n",
      "['-3.52, 1988/05/20 - 1988/06/24', '-3.15, 2001/04/15 - 2001/05/20', '-2.98, 2001/04/08 - 2001/05/13', '-2.97, 1998/05/13 - 1998/06/17', '-2.96, 1988/05/27 - 1988/07/01', '-2.94, 1993/04/15 - 1993/05/20', '-2.75, 1998/05/06 - 1998/06/10', '-2.52, 1991/05/06 - 1991/06/10', '-2.48, 1995/07/22 - 1995/08/26', '-2.36, 1993/04/22 - 1993/05/27']\n",
      "3 weeks\n",
      "['-3.04, 2001/04/15 - 2001/05/13', '-3.01, 1998/05/13 - 1998/06/10', '-2.67, 1988/05/27 - 1988/06/24', '-2.62, 1988/05/20 - 1988/06/17', '-2.56, 2001/04/22 - 2001/05/20', '-2.44, 1991/05/06 - 1991/06/03', '-2.24, 1993/04/15 - 1993/05/13', '-2.23, 1993/04/22 - 1993/05/20', '-2.21, 2001/04/08 - 2001/05/06', '-2.14, 1998/05/20 - 1998/06/17']\n",
      "2 weeks\n",
      "['-2.44, 2001/04/22 - 2001/05/13', '-2.26, 2001/04/15 - 2001/05/06', '-2.17, 1998/05/20 - 1998/06/10', '-2.15, 2021/05/06 - 2021/05/27', '-1.98, 1991/05/06 - 1991/05/27', '-1.95, 1988/05/20 - 1988/06/10', '-1.77, 1988/05/27 - 1988/06/17', '-1.73, 1998/05/13 - 1998/06/03', '-1.69, 1997/09/09 - 1997/09/30', '-1.60, 1991/05/13 - 1991/06/03']\n",
      "1 week\n",
      "['-1.67, 2001/04/22 - 2001/05/06', '-1.50, 1980/05/20 - 1980/06/03', '-1.35, 1999/05/27 - 1999/06/10', '-1.35, 2021/05/06 - 2021/05/20', '-1.34, 1992/05/13 - 1992/05/27', '-1.28, 1998/05/27 - 1998/06/10', '-1.19, 2006/09/16 - 2006/09/30', '-1.13, 1991/05/13 - 1991/05/27', '-1.12, 1995/08/12 - 1995/08/26', '-1.10, 1988/05/27 - 1988/06/10']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nonly april to september\\nevaporation, rainfall, streamflow\\naverage for box instead of just single coordinate-- DONE\\n90-274 (april to end of sept) -- DONE\\n\\ndates of top 10 dates (drops) for all 3 and see for matches for all 4 weeks\\nhighlight ones that match\\nstart documentation\\nsummarize functions\\nadd dictionary of variables\\n\\n\\nHOW TO ORGANIZE:\\nList top 10 streamflow events (list 10 separate dates, list all anyway)\\nList top 10 rainfall events\\nList top 10 evaporation events\\n\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Driver\n",
    "\n",
    "# Directory for output files\n",
    "outdir = \"outdir/\"\n",
    "\n",
    "'''\n",
    "\n",
    "Buffalo\n",
    "-78.75 to -79.0\n",
    "42.75 to 42.875\n",
    "\n",
    "Adirondacks\n",
    "-74.0 to -74.375\n",
    "43.625 to 43.875\n",
    "\n",
    "Catskills\n",
    "-74.5 to -74.5\n",
    "42.25 to 42.375\n",
    "\n",
    "Long Island\n",
    "-73.5 to -73.625\n",
    "40.625 to 40.625\n",
    "\n",
    "Buffalo is\n",
    "[-78.75, 42.75, 370, 81],\n",
    "[-78.75, 42.875, 370, 80],\n",
    "[-78.875, 42.75, 369, 81],\n",
    "[-78.875, 42.875, 369, 80],\n",
    "[-79, 42.75, 368, 81],\n",
    "[-79, 42.875, 368, 80]\n",
    "\n",
    "Adirondacks is \n",
    "[-74.0, 43.625, 408, 74],\n",
    "[-74.0, 43.75, 408, 73],\n",
    "[-74.0, 43.875, 408, 72],\n",
    "[-74.125, 43.625, 407, 74],\n",
    "[-74.125, 43.75, 407, 73],\n",
    "[-74.125, 43.875, 407, 72],\n",
    "[-74.25, 43.625, 406, 74],\n",
    "[-74.25, 43.75, 406, 73],\n",
    "[-74.25, 43.875, 406, 72],\n",
    "[-74.375, 43.625, 405, 74],\n",
    "[-74.375, 43.75, 405, 73],\n",
    "[-74.375, 43.875, 405, 72]\n",
    "\n",
    "Catskills is \n",
    "[-74.5, 42.25, 404, 85],\n",
    "[-74.5, 42.375, 404, 84]\n",
    "\n",
    "LONG ISLAND IS\n",
    "[-73.5, 40.625, 412, 98],\n",
    "[-73.625, 40.625, 411, 98]\n",
    "\n",
    "'''\n",
    "ryear, rmonth, rday = 2023, 2, 2\n",
    "percentile = 10\n",
    "scope = 7\n",
    "window = 4\n",
    "print(\"4 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 4)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"3 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 3)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"2 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 2)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"1 week\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 1)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "#for i in all_data:\n",
    "    #print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
