{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_comparison(graph, date, start_date, end_date, window, plotting, percentile, scope)\n",
    "\n",
    "import matplotlib as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "def sortkey2(e):\n",
    "    return e[1]\n",
    "\n",
    "def daily_no_avg(graph, start, date, end_date, start2, scope):\n",
    "    # returns list of that week average over all years\n",
    "    date_data = []\n",
    "    for date in list(graph.keys()):\n",
    "        date_data.append([dt.datetime(int(date[0:4]),int(date[5:7]),int(date[8:10])), graph[date]])\n",
    "    total_data = []\n",
    "    # just take first day and next first day\n",
    "    while start.year < int(end_date):\n",
    "        lst2 = []\n",
    "        accum = 0\n",
    "        for i in range(len(date_data)):\n",
    "            if date_data[i][0] >= start and date_data[i][0] <= start + dt.timedelta(days=scope):\n",
    "                lst2.append(date_data[i])\n",
    "                accum += date_data[i][1]\n",
    "        if len(lst2) >= 2:\n",
    "            total_data.append([start2.year, accum/len(lst2)])\n",
    "        start = dt.datetime(start.year + 1, start.month, start.day)\n",
    "        start2 = dt.datetime(start2.year + 1, start2.month, start2.day)\n",
    "    return total_data\n",
    "\n",
    "def graph_comparison(graph, date, start_date, end_date, window, plotting, percentile, scope):\n",
    "    if len(date) == 5:\n",
    "        start = dt.datetime(int(start_date), int(date[0:2]), int(date[3:5]))\n",
    "        start2 = start + dt.timedelta(weeks=window)\n",
    "        end2 = start2 + dt.timedelta(days=scope)\n",
    "    first_set = daily_no_avg(graph[0], start, date, end_date, start, scope)\n",
    "    second_set = daily_no_avg(graph[1], start2, date, end_date, start, scope)\n",
    "    plottable = []\n",
    "    for i in first_set:\n",
    "        for j in second_set:\n",
    "             if j[0] == i[0]:\n",
    "                  if i[1] != 0:\n",
    "                    plottable.append([i[0], j[1] - i[1]])\n",
    "    dct = {}\n",
    "    plottable.sort(key=sortkey2)\n",
    "    for i in range(len(plottable)):\n",
    "        if plotting:\n",
    "            dct[(str(plottable[i][0]))[2:4]] = plottable[i][1]\n",
    "        else:\n",
    "            dct[(str(plottable[i][0]))] = plottable[i][1]\n",
    "    if plotting:\n",
    "        plt.bar(dct.keys(), dct.values(), color = \"green\", width = 0.8)\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.title(\"Streamflow (%s) versus period after\" % (date))\n",
    "        plt.show()\n",
    "    else:\n",
    "        if len(dct) > 0:\n",
    "            if percentile != 0:\n",
    "                a = list(dct.values())\n",
    "                a = np.array(a)\n",
    "                p = np.percentile(a, percentile)  # return 50th percentile, i.e. median.\n",
    "                return p\n",
    "            else:\n",
    "                # [mean, median, interquartile range,standard deviation]\n",
    "                b = list(dct.values())\n",
    "                b = np.array(b)\n",
    "                return [list(dct.values()), 0, 0, 0, 0,start.strftime(\"%m/%d\"), end2.strftime(\"%m/%d\"), list(dct.keys())]\n",
    "                #return [list(dct.values()), np.mean(b), np.median(b), np.percentile(b, 75) - np.percentile(b, 25), np.std(b),start.strftime(\"%m/%d\"), end2.strftime(\"%m/%d\"), list(dct.keys())]\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using_all_weeks(date, end_date, percent, scope, window)\n",
    "def sortkey(e):\n",
    "    return e[0]\n",
    "\n",
    "def using_all_weeks(date, end_date, percent, scope, window):\n",
    "    start_date = 1980\n",
    "    # CHANGE THIS TO FILENAME VVVVVVVVVVVVVV\n",
    "    graph = json.load(open('{}soil_catskills.json'.format(outdir), 'r'))\n",
    "    date = dt.datetime(start_date, int(date[0:2]), int(date[3:5]))\n",
    "    total_data = []\n",
    "    date_data = []\n",
    "    for week in range(53):\n",
    "        date_data.append({})\n",
    "    for stat in graph:\n",
    "        new_date = dt.datetime(int(stat[0:4]),int(stat[5:7]),int(stat[8:10]))\n",
    "        date_data[((new_date.timetuple().tm_yday - 1) // 7)][stat] = graph[stat]\n",
    "    for week in range(52):\n",
    "        date2 = date + dt.timedelta(weeks=week)\n",
    "        # investigate looking across year\n",
    "        #Convert to string\n",
    "        mo = str(date2.month)\n",
    "        da = str(date2.day)\n",
    "        if len(mo) == 1:\n",
    "            mo = \"0\" + mo\n",
    "        if len(da) == 1:\n",
    "            da = '0' + da\n",
    "        temp1 = {}\n",
    "        temp2 = {}\n",
    "        temp1 = {**temp1,**date_data[week]}\n",
    "        if week + window < 52:\n",
    "            temp2 = {**temp2,**date_data[week + window]}\n",
    "        else:\n",
    "            temp2 = {**temp2,**date_data[(52 - week + window)]}\n",
    "        if scope // 7 > 1:\n",
    "            for additional_week in range((scope // 7) + 1):\n",
    "                if additional_week + week < 53:\n",
    "                    temp1 = {**temp1, **date_data[week + additional_week]}\n",
    "                else:\n",
    "                    temp1 = {**temp1, **date_data[week + additional_week - 52]}\n",
    "                if additional_week + week + (scope // 7) < 52:\n",
    "                    temp2 = {**temp2, **date_data[week + additional_week + (scope // 7)]}\n",
    "                else:\n",
    "                    temp2 = {**temp2, **date_data[week + additional_week - 52 + (scope // 7)]}\n",
    "        # COMBINE NECESSARY WEEKS FOR ANALYSIS\n",
    "        info = (graph_comparison([temp1, temp2], f\"{mo}-{da}\", start_date, end_date, window, False, 0, scope))\n",
    "        if info:\n",
    "            for i in range(len(info[0])):\n",
    "                total_data.append([info[0][i],f\"{info[7][i]}/{(info[5])} - {info[7][i]}/{info[6]}\"])\n",
    "    total_data.sort(key=sortkey)\n",
    "    templst = []\n",
    "    for i in (total_data[0:10]):\n",
    "        templst.append(f'{i[0]:.4f}, {i[1]}')\n",
    "    print(templst)\n",
    "    return total_data[0:(int(int(len(total_data) / (100 / percent))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph = json.load(open(\\'{}retro_timeseries.json\\'.format(\\'indir/\\'), \\'r\\'))\\nlst = [\"buffalo\", \"adirondacks\", \"catskills\", \"long_island\"]\\nfor region in lst:\\n    dct = {}\\n    for item in range(len(graph[region])):\\n        date = graph[\"dates\"][item]\\n        date2 = f\"{date[6:]}-{date[:2]}-{date[3:5]}\"\\n        dct[date2] = graph[region][item]\\n    outfilename = \\'%ssoil_%s.json\\'%(outdir, region)\\n    outfile = open(outfilename, \"w\")\\n    json.dump(dct, outfile)\\n    outfile.close()'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To convert retro_timeseries.json to a file readable by my code, un-comment and run this block\n",
    "\n",
    "'''graph = json.load(open('{}retro_timeseries.json'.format('indir/'), 'r'))\n",
    "lst = [\"buffalo\", \"adirondacks\", \"catskills\", \"long_island\"]\n",
    "for region in lst:\n",
    "    dct = {}\n",
    "    for item in range(len(graph[region])):\n",
    "        date = graph[\"dates\"][item]\n",
    "        date2 = f\"{date[6:]}-{date[:2]}-{date[3:5]}\"\n",
    "        dct[date2] = graph[region][item]\n",
    "    outfilename = '%ssoil_%s.json'%(outdir, region)\n",
    "    outfile = open(outfilename, \"w\")\n",
    "    json.dump(dct, outfile)\n",
    "    outfile.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing(data, end_date, percentile)\n",
    "MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', \"Dec\"]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graphing(data, end_date, percentile):\n",
    "    year_accum = []\n",
    "    year_accum2 = []\n",
    "    for i in range(int(end_date)-1980):\n",
    "        year_accum.append(0)\n",
    "        year_accum2.append(0)\n",
    "    month_accum = [0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for lst in data:\n",
    "            month_accum[int(lst[1][5:7]) - 1] += 1\n",
    "\n",
    "    plt.bar(MONTHS, month_accum, color =\"blue\")\n",
    "    plt.title(f\"Soil Moisture: {percentile} percent between 1979-{end_date}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Number of Occurances\")\n",
    "    plt.show()\n",
    "    if percentile == 10:\n",
    "        print(f\"1%: {data[int(len(data[0]) / 100)][0]:.4f}\")\n",
    "        print(f\"2%: {data[int(len(data[0]) / 50)][0]:.4f}\")\n",
    "        print(f\"5%: {data[int(len(data) / 20)][0]:.4f}\")\n",
    "        print(f\"10%: {data[int(len(data) / 10)][0]:.4f}\")\n",
    "\n",
    "        for lst in data[0:int(len(data) / (10))]:\n",
    "            year_accum[int(lst[1][0:4]) - int(1980)] += 1\n",
    "\n",
    "        plt.bar((range(int(1980), int(end_date))), year_accum, color =\"green\")\n",
    "        plt.title(f\"Occurances across years: 1 percent between {1980}-{end_date}\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Number of Occurances\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 weeks\n",
      "['-0.0474, 2012/06/10 - 2012/07/15', '-0.0441, 2002/06/17 - 2002/07/22', '-0.0441, 1988/05/20 - 1988/06/24', '-0.0433, 2012/06/17 - 2012/07/22', '-0.0407, 2011/06/24 - 2011/07/29', '-0.0390, 1988/06/10 - 1988/07/15', '-0.0380, 2002/06/10 - 2002/07/15', '-0.0370, 2016/06/03 - 2016/07/08', '-0.0369, 1998/07/08 - 1998/08/12', '-0.0366, 2012/06/03 - 2012/07/08']\n",
      "3 weeks\n",
      "['-0.0385, 2002/06/24 - 2002/07/22', '-0.0379, 2011/06/24 - 2011/07/22', '-0.0370, 2012/06/17 - 2012/07/15', '-0.0359, 2012/06/10 - 2012/07/08', '-0.0348, 2002/06/17 - 2002/07/15', '-0.0330, 1998/07/08 - 1998/08/05', '-0.0325, 2012/06/24 - 2012/07/22', '-0.0318, 1988/05/20 - 1988/06/17', '-0.0316, 1983/06/24 - 1983/07/22', '-0.0313, 1988/05/27 - 1988/06/24']\n",
      "2 weeks\n",
      "['-0.0292, 1984/05/27 - 1984/06/17', '-0.0291, 2002/06/24 - 2002/07/15', '-0.0270, 2011/07/01 - 2011/07/22', '-0.0262, 2012/06/24 - 2012/07/15', '-0.0260, 1991/06/10 - 1991/07/01', '-0.0259, 2011/06/24 - 2011/07/15', '-0.0255, 2012/06/17 - 2012/07/08', '-0.0250, 2021/05/06 - 2021/05/27', '-0.0248, 1983/07/01 - 1983/07/22', '-0.0247, 2003/06/17 - 2003/07/08']\n",
      "1 week\n",
      "['-0.0169, 1995/06/10 - 1995/06/24', '-0.0157, 1984/05/27 - 1984/06/10', '-0.0157, 2002/06/24 - 2002/07/08', '-0.0157, 2018/07/01 - 2018/07/15', '-0.0151, 1998/07/08 - 1998/07/22', '-0.0150, 2011/07/01 - 2011/07/15', '-0.0149, 1988/07/01 - 1988/07/15', '-0.0147, 2012/06/24 - 2012/07/08', '-0.0145, 2013/08/12 - 2013/08/26', '-0.0142, 1991/06/17 - 1991/07/01']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nonly april to september\\nevaporation, rainfall, streamflow\\naverage for box instead of just single coordinate-- DONE\\n90-274 (april to end of sept) -- DONE\\n\\ndates of top 10 dates (drops) for all 3 and see for matches for all 4 weeks\\nhighlight ones that match\\nstart documentation\\nsummarize functions\\nadd dictionary of variables\\n\\n\\nHOW TO ORGANIZE:\\nList top 10 streamflow events (list 10 separate dates, list all anyway)\\nList top 10 rainfall events\\nList top 10 evaporation events\\n\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Driver (same as evaporation)\n",
    "\n",
    "outdir = \"outdir/\"\n",
    "ryear, rmonth, rday = 2023, 2, 2\n",
    "percentile = 1\n",
    "scope = 7\n",
    "window = 4\n",
    "print(\"4 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 4)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"3 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 3)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"2 weeks\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 2)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "print(\"1 week\")\n",
    "all_data = using_all_weeks(\"01-01\", ryear, percentile, scope, 1)\n",
    "graphing(all_data, ryear, percentile)\n",
    "\n",
    "#for i in all_data:\n",
    "    #print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
